\chapter{Literature - Sensors, Processing, Networks}

%%%%%%%%%%%%%%%%%%%%%%%%%%    ASYNC SENSORS     %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Asynchronous Vision Sensors}  % 1 page
%Lots of different types of dynamic vision sensors \cite{delbruck2010activity}
%Why would we want a DVS? microparticle tracking \cite{ni2012asynchronous}
%Neuromorphic systems are becoming more wide spread \cite{delbruck2014research}
% Introduction to AVS 
%Introduce different kind of senssors which asynchronously fr
Asynchronous Vision Sesnors (AVS) offer a frame--free, event--driven approach to capturing vision information\cite{delbruck2010activity}. 
These sensors continuously output light intensity changes from a scene in the form of precisely timed (sub \ms) events. 
Each pixel in the sensor array acts independently based on the light in its receptive field resulting in a sparse, stimulus--based output. 
A recent spike in technology facilitated the development of many alternative asynchronous vision sensors with varying properties and speeds\cite{delbruck2014research}.
These systems are united in an attempt at emulating the efficient, high--speed, event--based spiking behaviour in biological vision systems\cite{delbruck2010activity}.
%Environments requiring low-power, low-latacy and dynamic range are well suited 
Asynchronous vision sensors are well suited to environments requiring dynamic range, low-power and low--latency sensing, such as microparticle tracking\cite{ni2012asynchronous}, robotics\cite{roboGoalie2013} or motion tracking and classification\cite{Lee2014, reverter2015neuromorphic}.

% How do they work
In contrast to traditional vision systems which densely sample the world at discrete time intervals, AVSs follow a stimulus driven paradigm only recording changes in the environment, meaning redundant information in the scene (e.g. stationary items or backgrounds) are not captured.  
The resulting data format, Address-Event Representation (AER)\cite{mahowald1992vlsi}, is in the form $\textless (x, y), t, p \textgreater$, where $(x, y)$ is the pixel location of a change, $t$ is the precise time of the change and $p$ is the polarity indicating if the light intensity change was brighter (positive event) or dimmer (negative event)i\cite{delbruck2010activity}. 
% TODO really should find a AER specific paper for this
This dramatically different data representation means new approaches to vision processing will be required to extract meaningful information from AER sensors\cite{akolkar2015can}. 

% They why. What are the advantages and disadvantages
Asynchronous vision sensors offer a new paradigm in which to approach capturing vision information, bringing with it many new opportunities and challenges. 
The low--redundancy asynchronous nature of these sensors allows low--power, low--computation processing in a more biologically realistic setting.



\subsection{Example sensors}
%Tobi made a camera \cite{delbruck2008}
%Discuss the Dynamic vision sensors, what they are capable of
%Used in stuff like \cite{delbruck2007fast}
%They also have a new camera called the DAVIS \cite{DAVIS}
The Dynamic Vision Sesnor (DVS) is an AVS capable of registering events to temporal precision of 15\us\cite{delbruck2008}. 
It has a 128x128 pixel array with \textgreater 120\textit{dB} dynamic range.
The Dynamic and Active Pixel Vision Sensor (DAVIS) is the newer model of the DVS with a 240x180 pixel array, 3\us temporal precision and 130\textit{dB} dynamic range\cite{DAVIS}. 
Additionally the DAVIS has circuitry (the active pixel sensor) enabling it to capture static scene illumination values like a standard camera.
Further an inbuilt inertial measurement unit (IMU) means movement information can be simultaneously recorded and used in processing. 
An alternative sensor is the asynchronous time-based image sensor (ATIS) with a temporal resolution of 10\us (at \textgreater100Lx)\cite{posch2011qvga}.
Like the DAVIS the ATIS is also capable of capturing scene illumination as well as events.  





%%%%%%%%%%%%%%%%%%%%%%%%    STANDARD VISION     %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Frame-based vision processing}   % 1 page
%Standard videos and frame based approaches

% Introduction to standard vision processing 
Standard vision processing has long been based on processing full 2D frames using well understood techniques such as Canny Edge detection\cite{canny1986computational}. 
Many techniques exist for various tasks such as face detection\cite{viola2004robust}, object tracking using kernels\cite{comaniciu2003kernel} or object classification\cite{krizhevsky2012imagenet}.
The method of these approaches varies significantly from gradient based computation in Canny Edge detection to kernel based object tracking to deep neural networks.
Yet they all make the same implicit assumption that vision information is temporally discretised into the frame-rate of the vision sensor used in recording. 

Frame-based techniques and associated applications are thus bound by the limitations of the recording device with respect to data speed and quality. 
In particular if a standard 30 fps camera is used a realtime system must wait 33ms between frames (excluding any processing time required). 
Using a higher frame--rate recording device has a bound on performance as this necessarily means there will be more data (much of which is redundant) to process, such that the processing time of the system becomes the bottle neck. 
Further image quality in frame-based systems is proportional to the amount of processing required to extract features.
Despite limitations from recording devices, many impressive results have come from these techniques, in particular deep neural networks\cite{krizhevsky2012imagenet}.
However the computation required for these state-of-the-art systems makes their use on low--power, low--computation devices impractical. 



%%%%%%%%%%%%%%%%%%%%%%%%    EVENT-BASED PROCESSING     %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\pagebreak
\section{Event-based processing}     % 2 page
% Intro to what event based processing is (using events)
Event-based data brings with it the need for radically different processing paradigms compared to frame-based approaches\cite{perez2013mapping, martin2015spiking, tan2015benchmarking}.
Conventional sensors generate large amounts of redundant data which is computationally expensive to process, making efficient event-based data an attractive option\cite{vanarse2016review}. 
Event-based processing is not limited to vision sensors however, as discussed in \cite{vanarse2016review} neuromorphic auditory and olfactory sensors also exist using the AER format. 
Challenges facing event-based vision sensors are shared among these other neuromorphic sensors as well as more common (and low cost) sensors (like IMUs for gait cycle measurments\cite{fida2015pre}).

Many standard machine learning techniques have implicit assumptions that data is discretised into uniform time samples or that temporal information is not present/important.
There have been attempts to integrate temporally rich data with standard frame-based approaches such as the recurrent Temporal Restricted Boltzmann Machine (TRBM)\cite{sutskever2009recurrent} to model motion capture data or using deep-belief networks with spiking systems\cite{Neftci2014, pedroni2013neuromorphic, OConnor2013}.
More suited to dealing with event-based data are the family of Spiking Neural Networks (SNN)\cite{henderson2015spike, perez2013mapping}.
SNNs are harder to train than the frame-based counterparts, leading many to train frame-based networks and convert the trained model into an equivilant spiking network at some small performance drop\cite{perez2013mapping, pedroni2013neuromorphic, OConnor2013}.

Event-based processing is not limited to neural networks and other techniques are being adapted or created to take advantage of these new highspeed sensors\cite{ni2015visual}.
A \textit{RoboGoalie} was created as an example application leveraging the low-latency sensor to stop fast moving ping-pong balls entering a goal\cite{delbruck2007fast}. 
A simple event clustering algorithm was sufficient to allow the RobotGoalie to track incoming balls and respond within 3\ms with a peak performance load of 4\% on a standard computer. 
This was conducted in a heavily constrained system though, where luminance was controlled, the DVS stationary and with a constant background. 
Strict weight, power and computational requirements of a quadrotor form a near-perfect environment in which traditional cameras fail and event-based sensors shine\cite{mueggler2014event}.
These low-power and low-computation features of event-based sensors make them particularly attractive for applications in mobile robotics and navigation\cite{weikersdorfer2013simultaneous, milford2015place}.
These constraints have led to focused efforts on developing efficent algorithms to calculate odometry \cite{censi2014low}, visual flow \cite{benosman2014event} and corner detection\cite{clady2015asynchronous}.

% More on applications here then next para will support why they make sense

The systems described have shown that through careful datastructures and representations, learning models are able to leverage temporal information from the precise spike timings of event-based data. 
It is well known the eye does not act as a traditional vision sensor capturing and transfering full frames to the brain but rather sends only relevant visual information in the form of spikes\cite{delbruck2010activity}.
In an attempt to mimic this spiking behaviour some have attempted to convert frame-based benchmark datasets into spiking equivilants using grayscale values to produce rate-coded spiking outputs, while others have suggested why this may be problematic \cite{akolkar2015can} and suggest guidelines to ensure dataset quality\cite{tan2015benchmarking}.
Studies have found that coding schemes using the precise timing of events may be more biologically realistic and account for situations where processing happens too quickly to be explained by rate-coded information transfer\cite{thorpe2001spike}. 
It was similarly found that the precise spike timings of events in event-based data has a significant contribution to the amount of information contained in the recording\cite{akolkar2015can}.
% NOTE: Thus precise spike timings are important so maybe using a screen is bad. 


%What other processing has already been done with them (things like Motion cones). 
%Example usage of DVS is fast motor control by \cite{delbruck2007fast}
%Discuss frame accumulation by other groups
%Example of event-based visial flow calculation by fitting local plans on incoming events\cite{benosman2014event} (what else is this lab doing?)
%Calculating odometry using event based information (application) \cite{censi2014low}
%event-based corner detection\cite{clady2015asynchronous}


\subsection{Visual information representations}

% The eye can process well
The ease with which biological systems can process visual information suggests it should be a key percept in artificial agents, but it is not.
% But uses funny data formats
The challenges limiting artificial vision systems may be in part due to the fundamental differences between the biological equivilant in both recording and data format.
% DVS is like the eye
Neuromorphic vision sensors have started to emerge closing the gap between artificial systems and biology\cite{mahowald1992vlsi}.
% and has similar data formats
These sensors record and output data which much more closely resembles the retina\cite{akolkar2015can}. 
% Could convert to Frames
Although more biologically realistic, the data formats from these sensors are fundamentally different to frame-based representations prohibiting standard processing techniques. 
Converting between events and frames can be as simple as collecting all events from a time period into a frame as in \cite{kogler2009bio, schraml2010dynamic} or involve more complex calcuations about the events usefulness\cite{mueggler2015lifetime}.
Inversely, there has been work converting from frames back to realistic spike-train\cite{afshar2013ripple} showing this conversion is not a simple mapping but contains losses\cite{akolkar2015can}. 
%   Not-bio but it works
Accumulating events into frames works with current processing paradigms but departs from biology which is likely using precise neuron firings as representations\cite{akolkar2015can}.
% If we want to process well we must learn to process data formats too
To achieve biological system level performance is may be necessary to preserve additional qualities of the biological data format and processing. 
% We have AER
Perhaps the most biologically realistic format considered for processing is the AER format \cite{mahowald1992vlsi}.
%   But this is limited in number algorithms
This relatively new format has few algorithms that can fully leverage the structure, some worthy mentions include a high-speed pencil balancing robot \cite{conradt2009pencil}, a robotic goalie \cite{roboGoalie2013} and scene reconstruction with super-pixel resolution\cite{kim2008simultaneous}.
%   Could use Spiking neural networks (but hard to train)
% Temporal surface
A middle ground between direct frame accumulation and AER formats is the use of a short term window in which events have some influence.
Memory surfaces are present in the literature in forms such as gaussians approximating position \cite{conradt2009pencil}, queues of events \cite{ni2012asynchronous} or as this work will investigate, functions of temporal difference \cite{afshar2016investigation}.


% 

%Disucss how this is more like the eye and the advantages of this model of processing \cite{mahowald1992vlsi}

%Interest in event-based processing and sensors stems from the analogies to biological vision and processing\cite{mahowald1992vlsi}. 
%Necessarily vision sensors more like biological retinas must use  essence of neural processing data structures which closely resemble real .

%Rumelhart's back prop might not actually be biologically realistic but effective in learning regardless \cite{Rumelhart1986}.
%Discusses the biological realism of using spikes \cite{akolkar2015can}
%What does this paper say about representations maybe bio isn't always an answer \cite{fida2015pre}

%Temporal surfaces \cite{afshar2016investigation}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  NEURAL NETWORKS   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Neural Networks}     % 1 page
% Neural networks are loosely based on the brain
Artificial neural networks have origins as software implementations of brain functions\cite{mcculloch1943logical}.
Several seminal breakthroughs, notably backpropagation (popularised by \cite{Rumelhart1986}), convolutional neural networks \cite{lecun1998gradient} and more recently deep learning \cite{schmidhuber2015deep} have made neural networks a powerful tool in many learning tasks. 
% Impressive performance (state of art) in image classification
In particular neural networks have become the state-of-the-art in many image recognition and classification tasks \cite{krizhevsky2012imagenet, szegedy2015going}.

% Composed of layers of processing units (neurons)
Artificial Neural networks are made up of many processing units (neurons) each which does a small computation and produces output. 
% Type and function of units affects network dynamics
The function that each individual processing unit computes can have a significant affect on the network performance (measured by a loss function). 
%   perceptron
The first, and perhaps simplest, processing unit is the perceptron which computes a binary classification based on whether a weighted sum of inputs excedes a threshold.
Perceptrons represent a simple base line from which to compare other unit activation functions.
Learning with a binary output can be troublesome and perceptrons can be augmented to accept real valued input and give real valued output.
These augmentations means the units will compute a linear function of the input.
%   Sigmoid
%More complex, non-linear, functions may be necessary in a le
Many challenging tasks cannot be solved using a linear transformation of the input leading to more complex, non-linear, activation functions being used.
In particular sigmoid (along with other bounded, nonconstant) activation functions have been shown to be universal approximatiors\cite{hornik1991approximation}.
%   ReLU
More recently Rectified Linear Units (ReLU) have become a popular non-linear unit\cite{lecun2015deep} for the faster training times and biological realism compared to sigmoid units \cite{glorot2011deep}.
%   More complex LSTM units exist but not used
Even more complex units with special properties such as memory in Long Short Term Memory units \cite{hochreiter1997long} could be used to keep a state in the network. 

% Specifying weights impossible so we use back prop and SGD to refine weights
%Adjusting internal parameters of the network cause it to compute different functions on the input, its performance can then be evaluated according to some loss function. 
A network simply computes some function on input producing output, for the network to solve a task the weights of each unit need to be set appropriately.
Setting these weights using human experts would not be possible due to complexity and number of units.
Instead learning rules can be used to adjust the weights in the network based on the networks performance on training examples (input and output pairs). 
% Loss functions play a big role
The network's performance on any given training example is measured by a loss function which the learning rule aims to minimise. 
A widely used learning rule is Stochastic Gradient Descent (S.G.D.) which adjust weights based on computing the steepest gradient using the backpropogation algorithm\cite{Rumelhart1986}.


\subsubsection{Autoencoders}
% What they do and how they work to clean data/form representations
Autoencoders are typically feed forward neural networks tasked with finding efficient data codings\cite{hinton1994autoencoders}.
A simple structure entails a single hidden layer which converts an input into a compressed code.
The activations of the hidden layer can then be converted back into a representation of the input through the weights of the output layer.  
That is, the input and output are identical so the network must learn to encode the dataset with the units in it's hidden layer. 
These networks are also useful for denoising datasets and dimensionality reduction\cite{hinton2006reducing}.


\subsubsection{Convolutional networks}
% Use in image processing 
Loosely analogous to receptive fields found in mammalian visual systems \cite{hubel1962receptive, hubel1968receptive}, units in convolutional nerual networks can focus on small spatial regions of the input for specific features. 
In standard networks weights must be specialised to extract a particular feature from a given location in an image.
If the weights have specialise and the input undergoes a spatial translation the weights will no longer be able to effeciently extract the feature.
Convolutional networks develop specialised visual feature detectors and use them repeatedly in a grid like pattern across the input to extract information\cite{lecun1998gradient}. 
This translational invariance means convolutional networks can create more feature detectors for the same number of weights as a standard network.
Using feature detectors in this way allows networks to ignore parts of the input while focusing on the current area. 
%Networks are also able to ignore background noise in an input and 
%By using the same feature detector at different locations on the same input commonly occuring features can be extracted from different spatial locations without the need for repeated weights 

%\subsection{Deeper networks}   % 1 page
%ResNet etc. % TODO find resnet reference
%ImageNet etc.
%This was some deep network work \cite{pedroni2013neuromorphic}
%Oconor used deep networks \cite{OConnor2013}

%\subsection{Spiking networks}    % 1 page
%O'Connors work on spiking deep belief networks \cite{OConnor2013}
%Using echo state networks to extract spatiotemporal features from event-based \cite{lagorce2015spatiotemporal}
%Move to spiking networks accompanied by converstions from 2D to 1D temporal trains \cite{afshar2013ripple}
%Discussion on 2D images and converting to spikes vs just using spiking sensor and the information preserved \cite{akolkar2015can}
%Paper on spiking networks for vision tasks, usese the DVS and compares to convNets \cite{martin2015spiking}
%Feeding a DVS output directly into spiking NN \cite{Bichler}
%Interesting learning rules for SNNs \cite{Bichler} not exaimined here though.

\subsection{Software frameworks}   % 1 page
Many software frameworks exist for implementing and running neural networks including; Caffe\cite{jia2014caffe}, Theano\cite{bastien2012theano}, Torch7\cite{collobert2011torch7} and Tensorflow\cite{abaditensorflow} plus many more.

\textbf{Caffe} is a deep learning framework created by the Berkeley Vision and Learning Center and released with a BSD 2-Clause license.
Caffe has GPU support, API's for the command line, python and matlab while underlying code base consists of C++ and CUDA.
Network structure and parameters are specified in Google's Protobuf format as Directed Aycyclic Graphs.

\textbf{Theano} is an python based computation library for mathmatical expressions that works closely with Numpy multi-dimensional arrays.
Networks are defined in python as computation graphs which can then implicitly use underlying C implementations or GPU routines for calculations. 
Theano is relased with a BSD 3-Clause license.

\textbf{Torch7} is a compution framework with a focus on GPU computation.
Calculations are specified in the LuaJIT programming language and calculations are done with underlying C and CUDA implementations. 
Torch7 is released under the GNU Public License.

\textbf{Tensorflow} is Google's numerical computation library with a focus on support for many different systems including CPU, GPU, mobile and embedded systems.
Computation graphs are specified in Python while calculations are done with underlying C and CUDA implementations. 

All of these frameworks have the required functionality for this project.
Theano and Torch7 have been competing for faster processing times \cite{bergstra2010theano, collobert2011torch7, bastien2012theano} in recent years securing them both processing times better than Tensorflow or Caffe\cite{bahrampour2015comparative}.
However as only shallow neural networks will be used optimised performance is not strictly needed.
Caffe requires a complex installation of many components/libraries and uses a less intuative model specification (protobuf). 
Torch7 also uses the less standard LuaJIT for model specification.
Tensorflow was indentified as the slowest but also as the most flexible framework.

\section{Neural Networks with event-based data}
With some exceptions, neural networks map a set of inputs to a set of outputs.
% Doesnt mesh well with DVS data
This paradigm does not mesh well with event-based data which has no natural segments to use as inputs as frame-based vision has frames. 
% Need different ways to incorporate the data 
Many different appraoches have been taken to convert event-based data into a format the network can accept but perhaps the most natural is the use of spiking neural networks. 
% We could try spiking nets naturally these make sense, oconnor, spiking convs and Amy
Unlike frame-based neural networks spiking neural networks use the timing of input to transfer information \cite{akolkar2015can}.
A set of spikes is called a spike train and spiking neural networks can learn to differentiate between individual or collections of spike trains. 
% Contrastive divergence (spiking version as well)
Despite this suitability for event-based data spiking neural networks are harder to train and require different learning rules \cite{walter2015neuromorphic, henderson2015spike}.
Some notable work converted a fully trained, frame-based, deep belief network into an event-based spiking neural network with less than 1\% drop in performance\cite{OConnor2013}. 
% Saeed convert from spiking to feature maps
%Using event-based data and feeding it into a network (Saeed) \cite{afshar2016investigation}
Alternatively spiking networks have been used as just a part of the processing to extract meaningful features before using a standard network for classification\cite{afshar2016investigation}.
Work has also been done trying to convert frame-based techniques like contrastive divergence \cite{Neftci2014} or backpropagation (SpikeProp) \cite{bohte2002error}.
Specific spike-based algorithms for extracting features from event-based sensors also exist \cite{Bichler} but are much less common than frame-based methods. 


\subsection{Benchmark datasets}
To fairly compare models it is necessary for the community to have standardised datasets\cite{barranco2016dataset, Gibson2014, tan2015benchmarking}.
Many datasets exist for frame-based models from well formatted and normalised datasets (such as MNIST\cite{lecun1998gradient}) to large complex datasets (such as ImageNet\cite{deng2009imagenet}).
In an attempt to compare event-based processing models to frame-based models some have used DVS recordings of the classic MNIST dataset, being either with stationary images and a moving sensor \cite{OConnor2013, orchard2015converting} or with a sationary sensor and moving images on a screen \cite{serrano2015poker, akolkar2015can}.
Notably different to others is the secondary digit dataset in \cite{akolkar2015can} in which a DVS performs vertical saccades while recording digits on a rotating cylinder.
However, in trying to be comparable with frame-based models these datasets fail to realise the full potential of the DVS to capture complex spatio-temporal surfaces because the simulus is inherently two dimensional.
Newer benchmark datasets have addressed this problem offering a wide range of stimuli, in particular complex real world self motion for a mobile robot \cite{Gibson2014, barranco2016dataset}.
There is still a deficency of extremely simple datasets around which strong foundations can be formed to allow a deeper understanding of network dynamics.
Further as capturing movement is a key advantage of the DVS over frame-based sensors this should be emphasised in datasets.


%Creates own plane dataset \cite{afshar2016investigation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%    SUMMARY    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Literature summary}      % 1 page
%Revisit each section in a sentance or two and link them all together.
% Asynchronous vision sensors, summarise pos, neg and diff and motive why we look at them
Asynchronous vision sensors offer a new approach to processing vision information. 
By capturing only the changes in light intensity the output becomes sparse and temporally rich.
% This work will only use the DVS
The Dynamic Vision Sensor (DVS) will be used as the source of event-based data for processing in this work.
% Standard vision processing has come a long way and has good techniques but bad assumptions
This data will be processed using traditional machine learning techniques, which have produced impressive results for frame-based visual information. 
% Event-based data is dramatically different
However these techniques make an implcit assumption that vision infomration is discretised into 2D frames which means they are not compatible with event-based data.
    % Doesnt work with standard vision techniques
% We can try and convert from DVS to frames but need to be careful 
In an attempt to leverage the existing vision processing literature the DVS output must be converted to a compatible format. 
The new format should preserve as much of the encoded temporal information as possible.

% Neurel networks are awesome at processing so lets use those, 
Various types of neural networks will be used to compare how changing parameters affects what can be learnt.
    % We should try different activations
Different neuron activations, namely linear, sigmoid and ReLU, may cause the network to learn differently if non-linearity is necessary. 
    % should try different structures, convolution for its bio inspired ness
More drastic structural changes like using convolutional neural networks may also be able to extract meaningful features using specialised feature detectors. 
    % We might also be able to use networks to clearn the data (autoenc)
Autoencoder networks could also be used to analyse encodings learnt in the hidden layers of networks or to denoise the dataset.
    % Several software frameworks examined, gonna use tf 
Implementation of these networks will be done in Tensorflow because of the flexibility of the framework, the slowly performance will not be a problem for the shallow networks used.
% A sensible apparoch to evnet-based is spiking nets but lets use frame-based anyways
Spiking neural networks will not be used in this project but form a logical next step for investigation of the representation power of event-based data. 
% But before we get started lets see what kind of datasets already exist for us to use...
However, to compare networks fairly a standard task and dataset must be established.
Current public dataset are replications of frame-based datasets or complex real world datasets, a simpler task needs to be established so network performance and dynamics can be carefully considered. 



