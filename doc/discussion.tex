\chapter{General discussion and conclusions}

\subsubsection{\textbf{TOTT: Event-based data accumulated into temporal surfaces implicitly encodes temporal information which can be leveraged by frame-based neural networks to make meaningful predictions.}}

\section{Major findings}
Chapter \ref{ch:attentional} demonstrates that frame-based shallow neural networks are able to leverage implicitly encoded temporal information to make meaningful predictions of future temporal surfaces. 
A network with direct linear connections between input and output units is capable of making predictions when the input 

- Shallow networks struggle with full scene temporal surfaces

- With attentional data a simple network can make meaningful predictions

- The used concolution networks could not learn the pattern but may just be a problem with the structure not the idea of conv nets

- Autoencoders can be used to help smooth data

- No need to run so many permutations of the networks, they did not reveal much insight and just wanted space, time and compute power. 


%% Significance of results



\section{Possible future work}
Should only be 2 or 3 points that are as big as honours/masters or PhD level problems. 

